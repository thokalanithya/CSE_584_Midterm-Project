# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OHz1RGP6Npkqp3V7vSXNzaKMfA8rlmgk
"""

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from torch.optim import Adam
import numpy as np
import torch.nn.functional as F

# 1. Load the dataset and check the column names
df = pd.read_csv('/content/drive/MyDrive/final_completed_text-4.csv')
print(df.columns)

# 2. Apply LabelEncoder to the 'model' column to generate numerical labels
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['model'])
print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))
print(df.head())

# 3. Dataset and Preprocessing

class LLMClassifierDataset(Dataset):
    def __init__(self, df, tokenizer, max_len):
        self.sentences_xi = df['original text'].values
        self.sentences_xj = df['completed text'].values
        self.labels = df['label'].values
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences_xi)

    def __getitem__(self, idx):
        xi = str(self.sentences_xi[idx])
        xj = str(self.sentences_xj[idx])
        label = self.labels[idx]

        # Encode the sentence pairs using the Roberta tokenizer
        inputs = self.tokenizer.encode_plus(
            xi,
            xj,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=True,
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': inputs['input_ids'].flatten(),
            'attention_mask': inputs['attention_mask'].flatten(),
            'token_type_ids': inputs['token_type_ids'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# 4. Custom Model with Activation Functions and Dropout
class CustomRobertaClassifier(nn.Module):
    def __init__(self, num_labels):
        super(CustomRobertaClassifier, self).__init__()
        self.roberta = RobertaModel.from_pretrained('roberta-base')
        self.dropout = nn.Dropout(0.3)  # Dropout to prevent overfitting
        self.relu = nn.ReLU()  # ReLU activation
        self.leaky_relu = nn.LeakyReLU()  # Leaky ReLU activation
        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs[1]  # Pooled output
        pooled_output = self.dropout(pooled_output)  # Dropout for regularization
        pooled_output = self.leaky_relu(pooled_output)  # Leaky ReLU for non-linearity
        logits = self.classifier(pooled_output)  # Final linear layer
        return logits

# 5. Tokenizer and Model Initialization
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
num_labels = 6  # Adjust based on your dataset
model = CustomRobertaClassifier(num_labels=num_labels)

# 6. Split into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)
test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Optimized batch size
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# 7. Optimizer, Learning Rate, and Scheduler
optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=1e-4)  # Learning rate and L2 regularization
total_steps = len(train_loader) * 10  # 10 epochs for increased training

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

# 8. Early Stopping Class
class EarlyStopping:
    def __init__(self, patience=3, delta=0):
        self.patience = patience
        self.delta = delta
        self.counter = 0
        self.best_loss = np.inf
        self.early_stop = False

    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

early_stopping = EarlyStopping(patience=3, delta=0.001)

# 9. Gradient Clipping
max_grad_norm = 1.0

# 10. Training Loop

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

loss_fn = nn.CrossEntropyLoss()

def train_model(model, data_loader, optimizer, scheduler, loss_fn, max_grad_norm):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total = 0

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = loss_fn(logits, labels)

        _, predictions = torch.max(F.softmax(logits, dim=1), dim=1)  # Use softmax
        correct_predictions += torch.sum(predictions == labels)
        total += labels.size(0)

        total_loss += loss.item()

        loss.backward()

        # Apply gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

        optimizer.step()
        scheduler.step()

    avg_loss = total_loss / len(data_loader)
    accuracy = correct_predictions.double() / total
    return avg_loss, accuracy.item()

def eval_model(model, data_loader, loss_fn):
    model.eval()
    total_loss = 0
    correct_predictions = 0
    total = 0

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = loss_fn(logits, labels)

            _, predictions = torch.max(F.softmax(logits, dim=1), dim=1)
            correct_predictions += torch.sum(predictions == labels)
            total += labels.size(0)

            total_loss += loss.item()

    avg_loss = total_loss / len(data_loader)
    accuracy = correct_predictions.double() / total
    return avg_loss, accuracy.item()

# 11. Training and Evaluation
epochs = 10  # Increased epochs
for epoch in range(epochs):
    # Training phase
    train_loss, train_accuracy = train_model(model, train_loader, optimizer, scheduler, loss_fn, max_grad_norm)

    # Validation phase
    val_loss, val_accuracy = eval_model(model, test_loader, loss_fn)

    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

    # Check early stopping
    early_stopping(val_loss)
    if early_stopping.early_stop:
        print("Early stopping triggered.")
        break