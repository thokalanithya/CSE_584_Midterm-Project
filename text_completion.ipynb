{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa3b27-9b49-47f2-af80-57fe0e1e4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, sent_tokenize\n",
    "import re\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pip install transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974d040-c21a-4ce6-97b6-08640594a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/hellaswag_train.jsonl', 'r') as file:\n",
    "    # Read and parse each line as JSON\n",
    "    training_data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce479e-35a6-4475-8294-acac1ff4b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting required data from dataset\n",
    "required_data = []\n",
    "data_key = 'ctx_a'\n",
    "\n",
    "for curr_data in training_data:\n",
    "  required_data.append(curr_data[data_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7751eea-89da-433e-87f6-e9e5874d83a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing: split the sentence at the verb\n",
    "def split_at_verb(sentence):\n",
    "    \"\"\"\n",
    "    Function to split a sentence at the first occurrence of a verb.\n",
    "    Returns the part before the verb and the part starting from the verb.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    for i, (word, pos) in enumerate(tagged_words):\n",
    "        # 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ' are POS tags for different verb forms\n",
    "        if pos.startswith('VB'):\n",
    "            # Split the sentence at the verb\n",
    "            before_verb = ' '.join(words[:i + 1])\n",
    "            from_verb = ' '.join(words[i + 1:])\n",
    "            return before_verb, from_verb\n",
    "\n",
    "    # If no verb is found, return the full sentence\n",
    "    return sentence, \"\"\n",
    "\n",
    "def preprocess_data(training_data):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by splitting into sentences\n",
    "    and splitting each sentence at the first verb.\n",
    "    \"\"\"\n",
    "    processed_training_data = []\n",
    "    for text in training_data:\n",
    "      sentences = sent_tokenize(text)\n",
    "\n",
    "      for sentence in sentences:\n",
    "          before_verb, from_verb = split_at_verb(sentence)\n",
    "          processed_training_data.append({\n",
    "              'before_verb': before_verb,\n",
    "              'from_verb': from_verb\n",
    "          })\n",
    "\n",
    "    return processed_training_data\n",
    "\n",
    "processed_data = preprocess_data(required_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520f10a5-fa7d-4cde-ac1a-5bf651bae1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing: Remove any full stops, duplicates and restrict sentence length.\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    seen_sentences = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Remove leading/trailing spaces\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        # Remove full stop at the end of the sentence\n",
    "        sentence = re.sub(r'\\.$', '', sentence)\n",
    "\n",
    "        # Split the sentence into words and check if it has at least 4 words\n",
    "        words = sentence.split()\n",
    "\n",
    "        if 5 <= len(words) and len(words) <= 12:\n",
    "            # Join words back to form the sentence (without extra spaces)\n",
    "            cleaned_sentence = ' '.join(words)\n",
    "\n",
    "            # Add the sentence only if it's not a duplicate\n",
    "            if cleaned_sentence not in seen_sentences:\n",
    "                processed_sentences.append(cleaned_sentence)\n",
    "                seen_sentences.add(cleaned_sentence)\n",
    "\n",
    "    return processed_sentences\n",
    "\n",
    "before_verb_sentences= []\n",
    "\n",
    "for data in processed_data:\n",
    "  before_verb_sentences.append(data['before_verb'])\n",
    "\n",
    "# Preprocess the sentences\n",
    "cleaned_sentences = preprocess_sentences(before_verb_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650dc6e-243d-4687-86bf-5d7a5872fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing: remove sentences with any characters other than alphabets\n",
    "def filter_sentences(sentences):\n",
    "    # Using list comprehension and regex to match sentences with only alphabetic characters and spaces\n",
    "    filtered_sentences = [sentence for sentence in sentences if re.match(r'^[A-Za-z\\s]+$', sentence)]\n",
    "    return filtered_sentences\n",
    "\n",
    "# Apply the filter\n",
    "cleaned_sentences = filter_sentences(cleaned_sentences)\n",
    "cleaned_sentences = cleaned_sentences[:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe72b0-6356-4344-9ce8-5924f24051a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final GPT-2 Model\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a padding token, so we assign it manually\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "completed_texts_gpt2 = []\n",
    "\n",
    "# Define the text generation function\n",
    "def generate_text(input_text):\n",
    "    # Encode the input, adding padding if necessary\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt', padding=True)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = (inputs != tokenizer.pad_token_id).long()\n",
    "\n",
    "    # Generate text completion with stopping at EOS token\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=30,  \n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,  # Lower temperature for more coherent output\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,  # Stop at the end-of-sequence token\n",
    "    )\n",
    "\n",
    "    # Decode and return the output text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    completed_texts_gpt2.append({input_text : generated_text})\n",
    "    return generated_text\n",
    "\n",
    "# Thread worker function\n",
    "def run_model_in_thread(input_sentence):\n",
    "    generate_text(input_sentence)\n",
    "\n",
    "# Example sentences for multi-threaded execution\n",
    "input_sentences = cleaned_sentences\n",
    "\n",
    "# Function to run up to 10 threads concurrently\n",
    "def run_with_limited_threads(sentences, max_threads=10):\n",
    "    threads = []\n",
    "\n",
    "    # Process sentences in batches of max_threads\n",
    "    for i in range(0, len(sentences), max_threads):\n",
    "        batch = sentences[i:i + max_threads]\n",
    "\n",
    "        # Create threads for the current batch\n",
    "        for sentence in batch:\n",
    "            thread = threading.Thread(target=run_model_in_thread, args=(sentence,))\n",
    "            threads.append(thread)\n",
    "\n",
    "        # Start the threads in the current batch\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for the threads in the current batch to complete\n",
    "        for thread in threads:\n",
    "            # print(\"thread joined\")\n",
    "            thread.join()\n",
    "\n",
    "        # Clear the thread list for the next batch\n",
    "        threads.clear()\n",
    "\n",
    "# Run the model using 10 threads at a time\n",
    "run_with_limited_threads(input_sentences, max_threads=10)\n",
    "\n",
    "print(\"All threads completed execution.\")\n",
    "\n",
    "with open('gpt2-completed-texts.txt', 'w') as file:\n",
    "    for sentence in completed_texts_gpt2:\n",
    "        file.write(str(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d61e2c-fb53-46ec-ac89-d3c152513881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final BLOOM model\n",
    "# Load the BLOOM model and tokenizer\n",
    "model_name = \"bigscience/bloom-560m\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "completed_text_bloom = []\n",
    "\n",
    "def complete_sentence(input_sentence, max_length=30):\n",
    "    # Tokenize the input sentence\n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors='pt')\n",
    "\n",
    "    # Generate text completion using the model\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
    "\n",
    "    # Decode the generated text\n",
    "    completed_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    completed_text_bloom.append({input_sentence : completed_text})\n",
    "    return completed_text\n",
    "\n",
    "# Thread worker function\n",
    "def run_model_in_thread(input_sentence):\n",
    "    complete_sentence(input_sentence)\n",
    "\n",
    "# Example sentences for multi-threaded execution\n",
    "input_sentences = cleaned_sentences\n",
    "\n",
    "# Function to run up to 10 threads concurrently\n",
    "def run_with_limited_threads(sentences, max_threads=10):\n",
    "    threads = []\n",
    "\n",
    "    # Process sentences in batches of max_threads\n",
    "    for i in range(0, len(sentences), max_threads):\n",
    "        batch = sentences[i:i + max_threads]\n",
    "\n",
    "        # Create threads for the current batch\n",
    "        for sentence in batch:\n",
    "            thread = threading.Thread(target=run_model_in_thread, args=(sentence,))\n",
    "            threads.append(thread)\n",
    "\n",
    "        # Start the threads in the current batch\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for the threads in the current batch to complete\n",
    "        for thread in threads:\n",
    "            # print(\"thread joined\")\n",
    "            thread.join()\n",
    "\n",
    "        # Clear the thread list for the next batch\n",
    "        threads.clear()\n",
    "\n",
    "# Run the model using 10 threads at a time\n",
    "run_with_limited_threads(input_sentences, max_threads=10)\n",
    "\n",
    "print(\"All threads completed execution.\")\n",
    "\n",
    "with open('bloom-completed-texts.txt', 'w') as file:\n",
    "    for sentence in completed_text_bloom:\n",
    "        file.write(str(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3579e6-4529-4793-9b64-c4c1c641885b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3ce87-4bec-4518-8140-c7e218e7869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final T5 Model\n",
    "# Load FLAN-T5 large model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "completed_texts_flan = []\n",
    "\n",
    "def complete_sentence_flan(prompt, max_length=30, num_return_sequences=1):\n",
    "    # Tokenize input text and create attention mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    # Generate output using the model, passing attention_mask\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,  # Pass the attention mask explicitly\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,      # Enable sampling for diverse outputs\n",
    "        top_k=50,            # Top-k sampling\n",
    "        top_p=0.95,          # Nucleus sampling\n",
    "        temperature=0.7,     # Control creativity\n",
    "        pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text into readable sentences\n",
    "    response = [tokenizer.decode(g, skip_special_tokens=True) for g in output]\n",
    "    appened = {prompt : prompt + \" \" + str(response[0])}\n",
    "    # print(appened)\n",
    "    completed_texts_flan.append(appened)\n",
    "    return response\n",
    "\n",
    "# Thread worker function\n",
    "def run_model_in_thread(input_sentence):\n",
    "    complete_sentence_flan(input_sentence, max_length=30, num_return_sequences=1)\n",
    "\n",
    "# Example sentences for multi-threaded execution\n",
    "input_sentences = cleaned_sentences\n",
    "\n",
    "\n",
    "# Function to run up to 10 threads concurrently\n",
    "def run_with_limited_threads(sentences, max_threads=10):\n",
    "    threads = []\n",
    "\n",
    "    # Process sentences in batches of max_threads\n",
    "    for i in range(0, len(sentences), max_threads):\n",
    "        batch = sentences[i:i + max_threads]\n",
    "\n",
    "        # Create threads for the current batch\n",
    "        for sentence in batch:\n",
    "            thread = threading.Thread(target=run_model_in_thread, args=(sentence,))\n",
    "            threads.append(thread)\n",
    "\n",
    "        # Start the threads in the current batch\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for the threads in the current batch to complete\n",
    "        for thread in threads:\n",
    "            # print(\"thread joined\")\n",
    "            thread.join()\n",
    "\n",
    "        # Clear the thread list for the next batch\n",
    "        threads.clear()\n",
    "\n",
    "# Run the model using 10 threads at a time\n",
    "run_with_limited_threads(input_sentences, max_threads=10)\n",
    "\n",
    "with open('flan-completed-texts.txt', 'w') as file:\n",
    "    for sentence in completed_texts_flan:\n",
    "        file.write(str(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b32d18-51da-4b44-b912-d5f3a2611ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e82d6-f3ee-4fb4-8c8c-105b71750fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176d666-97e1-4665-8e1d-25c6bb7ce8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292f358-14d8-434a-8a35-4154fe8bea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Neo GPT Model\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token to be the same as the EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "completed_texts_gptneo = []\n",
    "\n",
    "def complete_sentence_gpt_neo(prompt, max_length=50, num_return_sequences=1):\n",
    "    # Tokenize input text and create attention mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Generate output using the model\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,  # Pass the attention mask explicitly\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,       # Enable sampling for diverse outputs\n",
    "        top_k=50,             # Top-k sampling\n",
    "        top_p=0.95,           # Nucleus sampling for more balanced output\n",
    "        temperature=0.7,      # Control randomness\n",
    "        pad_token_id=tokenizer.pad_token_id  # Set pad_token_id to the newly assigned pad_token\n",
    "    )\n",
    "\n",
    "    # Decode the generated text into readable sentences\n",
    "    response = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    completed_texts_gptneo.append({prompt : response[0]})\n",
    "    return response\n",
    "\n",
    "\n",
    "# Thread worker function\n",
    "def run_model_in_thread(input_sentence):\n",
    "    complete_sentence_gpt_neo(input_sentence, max_length=30, num_return_sequences=1)\n",
    "\n",
    "# Example sentences for multi-threaded execution\n",
    "input_sentences = cleaned_sentences\n",
    "\n",
    "# Function to run up to 10 threads concurrently\n",
    "def run_with_limited_threads(sentences, max_threads=10):\n",
    "    threads = []\n",
    "\n",
    "    # Process sentences in batches of max_threads\n",
    "    for i in range(0, len(sentences), max_threads):\n",
    "        batch = sentences[i:i + max_threads]\n",
    "\n",
    "        # Create threads for the current batch\n",
    "        for sentence in batch:\n",
    "            thread = threading.Thread(target=run_model_in_thread, args=(sentence,))\n",
    "            threads.append(thread)\n",
    "\n",
    "        # Start the threads in the current batch\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for the threads in the current batch to complete\n",
    "        for thread in threads:\n",
    "            # print(\"thread joined\")\n",
    "            thread.join()\n",
    "\n",
    "        # Clear the thread list for the next batch\n",
    "        threads.clear()\n",
    "\n",
    "# Run the model using 10 threads at a time\n",
    "run_with_limited_threads(input_sentences, max_threads=10)\n",
    "\n",
    "with open('gptneo-completed-texts.txt', 'w') as file:\n",
    "    for sentence in completed_texts_gptneo:\n",
    "        file.write(str(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebaef0-be3f-4791-bc28-e78f455cccf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891f580-00b7-4ca2-a770-4c6b4413338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DialoGPT Model\n",
    "model_name = \"microsoft/DialoGPT-medium\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ensure you use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "completed_texts_dialogpt = []\n",
    "\n",
    "def complete_sentence_dialogpt(prompt, max_length=50, num_return_sequences=1):\n",
    "    # Tokenize input text and create attention mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Generate output using the model\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,       # Enable sampling for diverse outputs\n",
    "        top_k=50,             # Top-k sampling\n",
    "        top_p=0.95,           # Nucleus sampling for more balanced output\n",
    "        temperature=0.7,      # Control randomness\n",
    "        pad_token_id=tokenizer.pad_token_id  # Set pad_token_id to the newly assigned pad_token (which is EOS here)\n",
    "    )\n",
    "\n",
    "    # Decode the generated text into readable sentences\n",
    "    response = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    completed_texts_dialogpt.append({prompt : response[0]})\n",
    "    return response\n",
    "\n",
    "\n",
    "# Thread worker function\n",
    "def run_model_in_thread(input_sentence):\n",
    "    complete_sentence_dialogpt(input_sentence, max_length=30, num_return_sequences=1)\n",
    "\n",
    "input_sentences = cleaned_sentences\n",
    "\n",
    "\n",
    "# Function to run up to 10 threads concurrently\n",
    "def run_with_limited_threads(sentences, max_threads=10):\n",
    "    threads = []\n",
    "\n",
    "    # Process sentences in batches of max_threads\n",
    "    for i in range(0, len(sentences), max_threads):\n",
    "        batch = sentences[i:i + max_threads]\n",
    "\n",
    "        # Create threads for the current batch\n",
    "        for sentence in batch:\n",
    "            thread = threading.Thread(target=run_model_in_thread, args=(sentence,))\n",
    "            threads.append(thread)\n",
    "\n",
    "        # Start the threads in the current batch\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for the threads in the current batch to complete\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        # Clear the thread list for the next batch\n",
    "        threads.clear()\n",
    "\n",
    "# Run the model using 10 threads at a time\n",
    "run_with_limited_threads(input_sentences, max_threads=10)\n",
    "\n",
    "with open('dialogpt-completed-texts.txt', 'w') as file:\n",
    "    for sentence in completed_texts_dialogpt:\n",
    "        file.write(str(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077424a-4dc5-496c-bc41-d52f3d73b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data from all the models in a dataframe\n",
    "# Paths to all data files\n",
    "file_paths = ['/gpt2-completed-texts.txt', '/opt-completed-texts.txt', '/bloom-completed-texts.txt', '/flan-completed-texts.txt', '/gptneo-completed-texts.txt', '/dialogpt-completed-texts.txt']\n",
    "\n",
    "# Store all the data\n",
    "data_list = []\n",
    "\n",
    "# Open the file and read each line\n",
    "for file_path in file_paths:\n",
    "  with open(file_path, 'r') as file:\n",
    "      for line in file:\n",
    "          \n",
    "          # Create a dictionary for each line by splitting key-value pairs\n",
    "          key, value = line.split(':', 1)\n",
    "\n",
    "          if 'gpt2' in file_path:\n",
    "            model_name = 'GPT-2'\n",
    "          elif 'opt' in file_path:\n",
    "            model_name = 'OPT'\n",
    "          elif 'bloom' in file_path:\n",
    "            model_name = 'BLOOM'\n",
    "          elif 'flan' in file_path:\n",
    "            model_name = 'Flan-T5'\n",
    "          elif 'gptneo' in file_path:\n",
    "            model_name = 'GPT-Neo'\n",
    "          elif 'dialogpt' in file_path:\n",
    "            model_name = 'DialoGPT'\n",
    "\n",
    "          key = key.replace('{', '').replace('}', '').replace('\"', '').replace(\"'\", '')\n",
    "          value = value.replace('{', '').replace('}', '').replace('\"', '').replace(\"'\", '')\n",
    "          value = value.replace(key, '')\n",
    "\n",
    "          if len(value) > 0:\n",
    "            # Append the dictionary to the data list\n",
    "            data_list.append([key, value, model_name])\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(data_list, columns=['original text', 'completed text', 'model'])\n",
    "columns_to_export = ['original text', 'completed text', 'model']\n",
    "\n",
    "# Export the DataFrame to a CSV file, including only the specified columns\n",
    "df.to_csv('final_completed_text.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb07eab-c1a0-4e78-84be-d3cf2465ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and testing using Roberta classifier\n",
    "#INCREASED THE EPOCHS AND ADD EARLY STOPPING:\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv')\n",
    "print(df.columns)  # Print the columns to ensure correct column names\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])  # Create a new 'label' column\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  # Check the label mapping\n",
    "print(df.head())  # Print the first few rows to ensure 'label' column exists\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=6)  # Adjust 'num_labels' to match the number of unique LLMs\n",
    "\n",
    "# 5. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 6. Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 7. Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=3, delta=0.001)\n",
    "\n",
    "def train_model(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# 8. Training and Evaluation\n",
    "epochs = 10  # Increased epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopping(val_loss)\n",
    "\n",
    "    # If early stopping is triggered, stop training\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3664ce-c146-4d85-aa3d-3d8a2784bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing dropouts\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv') \n",
    "print(df.columns)  # Print the columns to ensure correct column names\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])  # Create a new 'label' column\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  # Check the label mapping\n",
    "print(df.head())  # Print the first few rows to ensure 'label' column exists\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Custom Model with Dropout Layers\n",
    "class CustomRobertaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomRobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout layer with a 30% dropout rate\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # Get the pooled output from the Roberta model\n",
    "        pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
    "        logits = self.classifier(pooled_output)  # Classification layer\n",
    "        return logits\n",
    "\n",
    "# Initialize the custom model with dropout layers\n",
    "num_labels = 6  # Adjust 'num_labels' to match the number of unique labels in your dataset\n",
    "model = CustomRobertaClassifier(num_labels)\n",
    "\n",
    "# 5. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# 6. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 7. Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 8. Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# 9. Training and Evaluation\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer, loss_fn)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01b57e-27e2-4514-979f-238576b484a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing learning rate\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv') \n",
    "print(df.columns)  # Print the columns to ensure correct column names\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])  # Create a new 'label' column\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  # Check the label mapping\n",
    "print(df.head())  # Print the first few rows to ensure 'label' column exists\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=6)  # Adjust 'num_labels' to match the number of unique LLMs\n",
    "\n",
    "# 5. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 6. Optimizer, Learning Rate, and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)  # Changed learning rate to 1e-5\n",
    "\n",
    "# Total steps for learning rate scheduler\n",
    "total_steps = len(train_loader) * 5  # 5 is the number of epochs\n",
    "\n",
    "# Scheduler to gradually decrease the learning rate\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,  # No warmup\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 7. Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, scheduler, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Adjust the learning rate after each batch\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# 8. Training and Evaluation\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer, scheduler, loss_fn)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165999d-4e0a-4668-b3c4-8c458a093662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing batch size\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv')\n",
    "print(df.columns)  # Print the columns to ensure correct column names\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])  # Create a new 'label' column\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  # Check the label mapping\n",
    "print(df.head())  # Print the first few rows to ensure 'label' column exists\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=6)  # Adjust 'num_labels' to match the number of unique LLMs\n",
    "\n",
    "# 5. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "# 6. Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 7. Variable Batch Sizes\n",
    "train_batch_size = 8   # Smaller batch size for training for more gradient updates\n",
    "valid_batch_size = 32  # Larger batch size for validation to speed up the process\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# Training and Evaluation Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer, loss_fn)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede16248-975f-43b5-b628-d100f11daaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented gradient clipping\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv')\n",
    "print(df.columns)  # Print the columns to ensure correct column names\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])  # Create a new 'label' column\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  # Check the label mapping\n",
    "print(df.head())  # Print the first few rows to ensure 'label' column exists\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=6)  # Adjust 'num_labels' to match the number of unique LLMs\n",
    "\n",
    "# 5. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 6. Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 7. Gradient clipping threshold\n",
    "max_grad_norm = 1.0  # Max norm for gradient clipping\n",
    "\n",
    "# 8. Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_fn, max_grad_norm):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# 9. Training and Evaluation\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer, loss_fn, max_grad_norm)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc83560-09dd-4ce4-953d-4abf8c3d6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax activation function\n",
    "\n",
    "#BASE MODEL\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv')\n",
    "print(df.columns)  # Print the columns to ensure correct column names\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])  # Create a new 'label' column\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  # Check the label mapping\n",
    "print(df.head())  # Print the first few rows to ensure 'label' column exists\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=6)  # Adjust 'num_labels' to match the number of unique LLMs\n",
    "\n",
    "# 5. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 6. Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 7. Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_model(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        _, predictions = torch.max(probs, dim=1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            _, predictions = torch.max(probs, dim=1)\n",
    "\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# 8. Training and Evaluation\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79da45-c3a8-4771-8886-7641390ccee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv') \n",
    "print(df.columns)  # Print the columns to ensure correct column names\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])  # Create a new 'label' column\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  # Check the label mapping\n",
    "print(df.head())  # Print the first few rows to ensure 'label' column exists\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Custom Model with ReLU Activation\n",
    "class CustomRobertaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomRobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.3)  # Add dropout to prevent overfitting\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # Use the pooled output from Roberta\n",
    "        pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
    "        pooled_output = self.relu(pooled_output)  # Apply ReLU activation\n",
    "        logits = self.classifier(pooled_output)  # Final classification layer\n",
    "        return logits\n",
    "\n",
    "# Initialize the custom model with ReLU\n",
    "num_labels = 6  # Adjust 'num_labels' to match the number of unique labels in your dataset\n",
    "model = CustomRobertaClassifier(num_labels)\n",
    "\n",
    "# 5. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# 6. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 7. Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 8. Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# 9. Training and Evaluation\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer, loss_fn)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dc56c-e6a3-4be9-a2d3-c7658b31c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv')  \n",
    "print(df.columns)  # Print the columns to ensure correct column names\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])  # Create a new 'label' column\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  # Check the label mapping\n",
    "print(df.head())  # Print the first few rows to ensure 'label' column exists\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # Use long for classification\n",
    "        }\n",
    "\n",
    "# 4. Custom Model (No Sigmoid/Softmax needed)\n",
    "class CustomRobertaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomRobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.3)  # Add dropout to prevent overfitting\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)  # Final classification layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # Use the pooled output from Roberta\n",
    "        pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
    "        logits = self.classifier(pooled_output)  # Final classification layer\n",
    "        return logits  # Return logits directly, no softmax/sigmoid needed\n",
    "\n",
    "# Initialize the custom model\n",
    "num_labels = 6  # Adjust 'num_labels' to match the number of unique labels in your dataset\n",
    "model = CustomRobertaClassifier(num_labels)\n",
    "\n",
    "# 5. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# 6. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 7. Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
    "\n",
    "# 8. Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        _, predictions = torch.max(logits, dim=1)  # Get the index of the max log-probability\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# 9. Training and Evaluation\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer, loss_fn)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978f7b6-bff2-4ad0-a1ac-8bbe1e798042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky relu\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv')\n",
    "print(df.columns)  # Print the columns to ensure correct column names\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])  # Create a new 'label' column\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  # Check the label mapping\n",
    "print(df.head())  # Print the first few rows to ensure 'label' column exists\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Custom Model with ReLU Activation\n",
    "class CustomRobertaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomRobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout layer to prevent overfitting\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # Use the pooled output from Roberta\n",
    "        pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
    "        pooled_output = self.relu(pooled_output)  # Apply ReLU activation\n",
    "        logits = self.classifier(pooled_output)  # Final classification layer\n",
    "        return logits\n",
    "\n",
    "# Initialize the custom model with ReLU\n",
    "num_labels = 6  # Adjust 'num_labels' to match the number of unique labels in your dataset\n",
    "model = CustomRobertaClassifier(num_labels)\n",
    "\n",
    "# 5. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# 6. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 7. Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 8. Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# 9. Training and Evaluation\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer, loss_fn)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93286d8a-2f3c-4301-995c-36f079b28165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL OPTIMISED MODEL\n",
    "\n",
    "# 1. Load the dataset and check the column names\n",
    "df = pd.read_csv('/final_completed_text.csv')\n",
    "print(df.columns)\n",
    "\n",
    "# 2. Apply LabelEncoder to the 'model' column to generate numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['model'])\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "print(df.head())\n",
    "\n",
    "# 3. Dataset and Preprocessing\n",
    "\n",
    "class LLMClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.sentences_xi = df['original text'].values\n",
    "        self.sentences_xj = df['completed text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_xi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xi = str(self.sentences_xi[idx])\n",
    "        xj = str(self.sentences_xj[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the sentence pairs using the Roberta tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            xi,\n",
    "            xj,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Custom Model with Activation Functions and Dropout\n",
    "class CustomRobertaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomRobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout to prevent overfitting\n",
    "        self.relu = nn.ReLU()  # ReLU activation\n",
    "        self.leaky_relu = nn.LeakyReLU()  # Leaky ReLU activation\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # Pooled output\n",
    "        pooled_output = self.dropout(pooled_output)  # Dropout for regularization\n",
    "        pooled_output = self.leaky_relu(pooled_output)  # Leaky ReLU for non-linearity\n",
    "        logits = self.classifier(pooled_output)  # Final linear layer\n",
    "        return logits\n",
    "\n",
    "# 5. Tokenizer and Model Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "num_labels = 6  # Adjust based on your dataset\n",
    "model = CustomRobertaClassifier(num_labels=num_labels)\n",
    "\n",
    "# 6. Split into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = LLMClassifierDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = LLMClassifierDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Optimized batch size\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 7. Optimizer, Learning Rate, and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=1e-4)  # Learning rate and L2 regularization\n",
    "total_steps = len(train_loader) * 10  # 10 epochs for increased training\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# 8. Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, delta=0.001)\n",
    "\n",
    "# 9. Gradient Clipping\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# 10. Training Loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, data_loader, optimizer, scheduler, loss_fn, max_grad_norm):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        _, predictions = torch.max(F.softmax(logits, dim=1), dim=1)  # Use softmax\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            _, predictions = torch.max(F.softmax(logits, dim=1), dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "# 11. Training and Evaluation\n",
    "epochs = 10  # Increased epochs\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_accuracy = train_model(model, train_loader, optimizer, scheduler, loss_fn, max_grad_norm)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_accuracy = eval_model(model, test_loader, loss_fn)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Check early stopping\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
